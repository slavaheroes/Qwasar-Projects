{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## Part 1\n",
    "def h(x, theta):\n",
    "    return x.dot(theta)\n",
    "\n",
    "def mean_squared_error(y_predicted, y_label):\n",
    "    return (1.0/y_label.size) * np.sum(np.square(y_predicted-y_label))\n",
    "\n",
    "## Part 2\n",
    "## Write a class LeastSquareRegression to calculate the Î¸ feature weights and make predictions.\n",
    "\n",
    "class LeastSquaresRegression():\n",
    "    def __init__(self,):\n",
    "        self.theta_ = None  \n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # Calculates theta that minimizes the MSE and updates self.theta_\n",
    "        inv = np.linalg.inv(np.dot(X.T, X))\n",
    "        self.theta_ = np.dot(np.dot(inv, X.T), y)\n",
    "\n",
    "        \n",
    "    def predict(self, X):\n",
    "        # Make predictions for data X, i.e output y = h(X) (See equation in Introduction)\n",
    "        return h(X, self.theta_)\n",
    "\n",
    "X = 4 * np.random.rand(100, 1)\n",
    "y = 10 + 2 * X + np.random.randn(100, 1)\n",
    "\n",
    "def bias_column(X):\n",
    "    z = np.ones((X.size, 1))\n",
    "    return np.append(z, X, axis=1)\n",
    "    \n",
    "\n",
    "X_new = bias_column(X)\n",
    "\n",
    "model = LeastSquaresRegression()\n",
    "model.fit(X_new, y)\n",
    "\n",
    "print(\"Theta my Normal Eq.: \" + str(model.theta_))\n",
    "\n",
    "y_new = model.predict(X_new)\n",
    "\n",
    "def my_plot(X, y, y_new):\n",
    "    plt.scatter(X, y)\n",
    "    plt.plot(X, y_new, color='red')\n",
    "    plt.show()\n",
    "\n",
    "my_plot(X, y, y_new)\n",
    "\n",
    "## Part 3\n",
    "\n",
    "class GradientDescentOptimizer():\n",
    "\n",
    "    def __init__(self, f, fprime, start, learning_rate = 0.001):\n",
    "        self.f_      = f                       # The function\n",
    "        self.fprime_ = fprime                  # The gradient of f\n",
    "        self.current_ = start                  # The current point being evaluated\n",
    "        self.learning_rate_ = learning_rate    # Does this need a comment ?\n",
    "\n",
    "        # Save history as attributes\n",
    "        self.history_ = [start]\n",
    "    \n",
    "    def step(self):\n",
    "        # Take a gradient descent step\n",
    "        # 1. Compute the new value and update selt.current_\n",
    "        # 2. Append the new value to history\n",
    "        # Does not return anything\n",
    "        self.current_ = self.current_ - self.learning_rate_ * self.fprime_(self.current_)\n",
    "        self.history_.append(self.current_)\n",
    "\n",
    "    def optimize(self, iterations = 100):\n",
    "        # Use the gradient descent to get closer to the minimum:\n",
    "        # For each iteration, take a gradient step\n",
    "        for i in range(iterations):\n",
    "            self.step()\n",
    "\n",
    "            \n",
    "    def print_result(self):\n",
    "        print(\"Best theta found is \" + str(self.current_))\n",
    "        print(\"Value of f at this theta: f(theta) = \" + str(self.f_(self.current_)))\n",
    "        print(\"Value of f prime at this theta: f'(theta) = \" + str(self.fprime_(self.current_)))\n",
    "\n",
    "def f(x):\n",
    "    return 3 + ((x[0]-2)**2)*((x[1]-6)**2)\n",
    "\n",
    "def fprime(x):\n",
    "    return 2*np.array([(x[0]-2)*(x[1]-6)*(x[1]-6), (x[0]-2)*(x[0]-2)*(x[1]-6)])\n",
    "\n",
    "grad = GradientDescentOptimizer(f, fprime, np.random.normal(size=(2,)), 0.001)\n",
    "grad.optimize()\n",
    "grad.print_result()\n",
    "\n",
    "def myplot3D(history):\n",
    "    x1 = np.arange(-5,5,0.01)\n",
    "    x2 = np.arange(-5,5,0.01)\n",
    "    X,Y = np.meshgrid(x1, x2)\n",
    "    Z = 3 + np.square(X-2)*np.square(Y-6)\n",
    "\n",
    "    fig = plt.figure(figsize=(10,8))\n",
    "    x_theta = np.array([x[0] for x in history])\n",
    "    y_theta = np.array([x[1] for x in history])\n",
    "    z_theta = 3 + np.multiply(np.square(x_theta-2), np.square(y_theta-6))\n",
    "\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.plot_surface(X,Y,Z, color='b', alpha=0.2)\n",
    "    ax.plot(x_theta, y_theta, z_theta, color='green')\n",
    "    ax.scatter(x_theta, y_theta, z_theta, color='red')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "myplot3D(grad.history_)\n",
    "\n",
    "## Part 4\n",
    "\n",
    "theta_start = np.random.randn(2,1)\n",
    "def gradient_descent(X, m, y, theta_start, iterations = 100, learning_rate = 0.1):\n",
    "    for _ in range(iterations):\n",
    "        grad = (2/m) * np.matmul(X.T, np.matmul(X, theta_start) - y)\n",
    "        theta_start = theta_start - learning_rate*grad\n",
    "    \n",
    "    return theta_start\n",
    "\n",
    "    \n",
    "\n",
    "theta001 = gradient_descent(X_new, 100, y, theta_start, learning_rate=0.01)\n",
    "theta01 = gradient_descent(X_new, 100, y, theta_start)\n",
    "theta07 = gradient_descent(X_new, 100, y, theta_start, learning_rate=0.7)\n",
    "\n",
    "print(\"Theta at learning rate = 0.01: \"+ str(theta001))\n",
    "print(\"Theta at learning rate = 0.1: \"+ str(theta01))\n",
    "print(\"Theta at learning rate = 0.7: \"+ str(theta07))\n",
    "\n",
    "y_predicted001 = h(X_new, theta001)\n",
    "y_predicted01 = h(X_new, theta01)\n",
    "y_predicted07 = h(X_new, theta07)\n",
    "\n",
    "plt.plot(X, y, \"b.\",label=\"Data points\")\n",
    "plt.plot(X, y_predicted001, \"r-\", label = \"l_rate = 0.01\")\n",
    "plt.plot(X, y_predicted01, \"g-\", label=\"l_rate = 0.1\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.title(\"Gradient descent output for diff. learning rates\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.plot(X, y, \"b.\",label=\"Data points\")\n",
    "plt.plot(X, y_predicted07, \"c-\", label=\"l_rate=0.7\")\n",
    "plt.legend()\n",
    "plt.title(\"Gradient descent output for diff. learning rates\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "acff4dd298a687c1fd5bb40cae5a90bd5e1d984e4de4bd70b9f3aab7caa94ee6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}