# -*- coding: utf-8 -*-
"""MyIrisDataset.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cW_q7PvY7mgzbbUTPe5dXnM441uOf2iS

Part I Load data
Create a function load_dataset(), you will load the dataset and returns it.
"""

import requests
import pandas as pd

def load_dataset():
  url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'
  r = requests.get(url)

  with open('iris.data', 'w') as f:
      f.write(r.text)

  with open('iris.data') as f:
    iris_data = pd.read_csv(f, header = None)

  iris_data.columns=['sepal_len', 'sepal_wid', 'petal_len', 'petal_wid', 'class']
  return iris_data

iris_data = load_dataset()
print(iris_data)

"""Part II Summarizing the dataset
Summarizing the dataset:
Create a function summarize_dataset(), it will print (in this order):
"""

def summarize_dataset():
  print(f' shape is {iris_data.shape}')
  print(f'\nits first 10 lines\n {iris_data.head(10)}')
  print(f'\nstatistical summary \n')
  print(iris_data.describe())
  print("\ndistribution\n")
  print(iris_data.groupby(by = ['class']).size())

summarize_dataset()

"""#A - Univariate"""

from matplotlib import pyplot

def print_plot_univariate():
  iris_data.hist()
  pyplot.show()


print_plot_univariate()

"""#B - Multivariate"""

from pandas.plotting import scatter_matrix

def print_plot_multivariate():
  scatter_matrix(iris_data)
  pyplot.show()

print_plot_multivariate()

from sklearn.model_selection import cross_val_score
from sklearn.model_selection import train_test_split
from sklearn.model_selection import KFold
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.svm import SVC

def my_print_and_test_models():
    array = iris_data.values
    X = array[:,0:4]
    y = array[:,4]
    X_train, X_validation, Y_train, Y_validation = train_test_split(X, y, test_size=0.2, random_state=1)

    kfold = KFold()

    # Decision Tree
    model_name = 'DecisionTree'
    model = DecisionTreeClassifier()
    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring='accuracy')
    print('%s: %f (%f)' % (model_name, cv_results.mean(), cv_results.std()))

    # GaussianNB
    model_name = 'GaussianNB'
    model = GaussianNB()
    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring='accuracy')
    print('%s: %f (%f)' % (model_name, cv_results.mean(), cv_results.std()))

    # KNeighbors
    model_name = 'KNeighbors'
    model = KNeighborsClassifier()
    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring='accuracy')
    print('%s: %f (%f)' % (model_name, cv_results.mean(), cv_results.std()))

    # LogisticRegression
    model_name = 'LogisticRegression'
    model = LogisticRegression(solver='liblinear', multi_class='ovr')
    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring='accuracy')
    print('%s: %f (%f)' % (model_name, cv_results.mean(), cv_results.std()))
    
    # LinearDiscriminant
    model_name = 'LinearDiscriminant'
    model = LinearDiscriminantAnalysis()
    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring='accuracy')
    print('%s: %f (%f)' % (model_name, cv_results.mean(), cv_results.std()))
    
    # SVM
    model_name = 'SVM'
    model = SVC(gamma='auto')
    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring='accuracy')
    print('%s: %f (%f)' % (model_name, cv_results.mean(), cv_results.std()))
    

my_print_and_test_models()

